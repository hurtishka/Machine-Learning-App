{% import "bootstrap/wtf.html" as wtf %}
{% extends 'bootstrap/base.html' %}

{% block head %}
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>MACHINE LEARNING ALGORITHMS</title>
<link href="//cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.5.3/css/bootstrap.min.css" rel="stylesheet">
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
<link rel="stylesheet" type="text/css" href="static/css/algorithms.css">
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.1/es5/tex-mml-chtml.js"></script>
<script type="text/javascript" src="https://yastatic.net/jquery/2.1.3/jquery.min.js"></script>
{% endblock %}

{% block body %}
<section id="welcome">
    <div class="container">
        <nav class="navbar navbar-expand-md navbar-dark">
            <div class="container-fluid">
                <a class="navbar-brand" href="/">
                    <img src="/static/img/logo_new.png" alt="" class="d-inline-block align-text-center">
                </a>
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
                    <span class="navbar-toggler-icon"></span>
                </button>
                <div class="collapse navbar-collapse">
                    <ul class="navbar-nav me-auto mb-2 mb-lg-0">
                        <li class="nav-item">
                            <a class="nav-link" href="/">На главную</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="algorithms">Алгоритмы</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="classification">Классификация</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="train">Обучение</a>
                        </li>
                    </ul>
                </div>
            </div>
        </nav>
</section>

<section id="algorithm-wl" style="padding-top: 2.5rem;">
    <div class="container">
        <div class="row">
            <div class="col-7 align-self-start">
                <p class="title-dt" style="font-size: 3.5rem;">Алгоритмы машинного обучения с учителем</p>
                <div class="description-dt">
                    <p style="text-indent: 2rem;">Обучение с учителем (англ. Supervised learning) — один из способов машинного обучения, в ходе которого испытуемая система принудительно обучается с помощью примеров «стимул-реакция». С точки зрения кибернетики, является одним из видов кибернетического эксперимента.</p>
                    <p style="text-indent: 2rem;">Пусть имеется множество объектов (ситуаций) и множество возможных ответов (откликов, реакций). Существует некоторая зависимость между ответами и объектами, но она неизвестна. Известна только конечная совокупность прецедентов – пар «объект, ответ», называемая обучающей выборкой.</p> 
                    <p style="text-indent: 2rem;">На основе этих данных требуется восстановить зависимость, то есть построить алгоритм, способный для любого объекта выдать достаточно точный ответ. Для измерения точности ответов определённым образом вводится функционал качества. Под учителем понимается либо сама обучающая выборка, либо тот, кто указал на заданных объектах правильные ответы.</p>
                </div>
            </div>
            <div class="col-5 align-self-center">
                <img src="../static/img/ML.png" alt=""  class="img-fluid">
            </div>
        </div>
    </div>
    <nav class="navbar" style="padding-top: 8rem;">
        <div class="container" style="padding-bottom: 4rem;">
          <a style="color: black;" class="navbar-brand" href="#algorithm-dt"><button type="button" class="btn btn-dark btn-lg">Decision Tree</button></a>
          <a style="color: black;" class="navbar-brand" href="#algorithm-knn"><button type="button" class="btn btn-dark btn-lg">k-Nearest Neighbors</button></a>
          <a style="color: black;" class="navbar-brand" href="#algorithm-nb"><button type="button" class="btn btn-dark btn-lg">Naive Bayes</button></a>
          <a style="color: black;" class="navbar-brand" href="#algorithm-svm"><button type="button" class="btn btn-dark btn-lg">Support Vector Machine</button></a>
          <a style="color: black;" class="navbar-brand" href="#algorithm-lr"><button type="button" class="btn btn-dark btn-lg">Logistic Regression</button></a>
          <hr width="100%" color="#000000"/>
        </div>
      </nav>
</section>

<section id="algorithm-dt">
    <div class="container">
        <div class="row">
            <div class="col-7 align-self-start">
                <p class="title-dt">Decision Tree — Дерево принятия решений</p>
                <div class="description-dt">
                    <p style="text-indent: 2rem;">Дерево принятия решений – это классификатор, построенный на основе решающих правил вида «если...то...», упорядоченных в древовидную иерархическую структуру. В основе работы дерева решений лежит процесс рекурсивного разбиения исходного множества объектов на подмножества, ассоциированные с предварительно заданными классами.</p> 
                    <p style="text-indent: 2rem;">Разбиение производится с помощью решающих правил, в которых осуществляется проверка значений атрибутов по заданному условию. В качестве обучающего набора данных используется множество наблюдений, для которых предварительно задана метка класса.</p>
                    <p style="text-indent: 2rem;">Дерево решений состоит из узлов (node), ветвей (branch) и листьев (leaf). В вершине дерева находится корневой узел, содержащий входные данные, с которого начинается построение. Далее идут внутренние узлы, каждый из которых имеет только одну входящую в него ветвь. В этих узлах осуществляется разделение входных значений на подмножества, называемые классами, в соответствии с их атрибутами. В ветвях хранятся значения атрибутов, по которым разбиваются данные. Лист представляет собой узел решения о принадлежности к классу.</p> 
                    <p style="text-indent: 2rem;">Процесс разбиения продолжается до тех пор, пока не будет достигнут необходимый результат расщепления или критерий остановки.</p>
                    <p style="text-indent: 2rem;">1.	Критерий Gain. Пусть дано множество A, состоящее из n элементов m, из которых обладают свойством S. Пусть множество A классифицировано посредством атрибута Q, имеющего q возможных значений. Тогда прирост информации Gain(A,Q) определяется как: $$Gain(A,Q)=H(A,S)-\sum_{i=1}^q\frac{|A_i|}{|A|}H(A_i,S) ,$$</p> 
                    <p>где H(A,S) – энтропия множества A по отношению к свойству S. Энтропия H(A,S) вычисляется следующим образом: $$H(A,S)=-\sum_{i=1}^S\frac{m_i}{n}\log\frac{m_i}{n}$$</p> 
                    <p style="text-indent: 2rem;">На каждом шаге алгоритм должен выбрать тот атрибут, для которого прирост информации максимален.</p> 
                    <p style="text-indent: 2rem;">2.	Критерий GainRatio. Данный критерий учитывает не только количество информации, требуемое для записи результата, но и количе-ство информации, требуемое для разделения по атрибуту: $$GainRatio(A,Q)=\frac{Gain(A,Q)}{SplitInfo(A,Q)} ,$$</p> 
                    <p>где SplitInfo(A,Q) – величина, оценивающая потенциальную информацию, получаемую при разбиении множества A на q подмножеств: $$SplitInfo(A,Q)=-\sum_{i=1}^q\frac{|A_q|}{|A|}\log_2\frac{|A_q|}{|A|} .$$</p> 
                    <p style="text-indent: 2rem;">3.	Индекс Gini. Его суть состоит в определении вероятности «нечистоты» в узле, т.е. вероятность того, что выбранный элемент будет неправильно помечен и отнесен не к тому классу. Для набора тестов A и свойства S, имеющего s значений, индекс Gini вычисляется следующим образом: $$Gini(A,S)=1-\sum_{i=1}^q\frac{|A_q|}{|A|} .$$</p> 
                    <p style="text-indent: 2rem;">Построение оптимального дерева является трудной задачей. Преимуществом деревьев является быстрое обучение и точное предсказание.</p> 
                </div>
            </div>
            <div class="col-5 align-self-center">
                <img src="../static/img/DT.png" alt=""  class="img-fluid">
                <img src="../static/img/DT.png" alt=""  class="img-fluid">
                <img src="../static/img/DT.png" alt=""  class="img-fluid">
            </div>
        </div>
        <div class="row text-center">
            <hr width="100%" color="#000000"/>
            <div class="col-7 text-center" style="margin: 0 auto;">
                <p class="title-small">Плюсы</p>
                    <p class="decr" style="text-indent: 2rem;">\(\bullet\) Их легко понять. В каждом узле мы можем точно увидеть, какое решение принимает наша модель. На практике мы сможем точно узнать, откуда исходят точности и ошибки, с какими видами данных модель будет справляться и как значения признаков влияют на выход. Опция визуализация в Scikit-learn является удобным инструментом, способствующим хорошему пониманию деревьев решений.</p> 
                    <p class="decr" style="text-indent: 2rem;">\(\bullet\) Не требует объемной подготовки данных. Многие модели машинного обучения требуют предварительной обработки данных (например, нормализации) и нуждаются в сложных схемах регуляризации. С другой стороны, деревья решений эффективны после настройки некоторых параметров.</p> 
                    <p class="decr" style="text-indent: 2rem;">\(\bullet\) Стоимость использования дерева для вывода является логарифмической от числа точек данных, используемых для обучения дерева. Это является большим преимуществом, так как большое количество данных не сильно повлияет на скорость вывода.</p> 
            </div>
            <div class="col-5 text-center" style="margin: 0 auto;">
                <p class="title-small">Минусы</p>
                    <p class="decr" style="text-indent: 2rem;">\(\bullet\) Из-за своего характера обучения деревья решений подвержены переобучению. Рекомендуется часто применять некоторые виды понижения размерности, например, PCA, чтобы дерево не создавало разбиения по большому количеству признаков.</p> 
                    <p class="decr" style="text-indent: 2rem;">\(\bullet\) По тем же причинам, что и переобучение, деревья решений также уязвимы к смещению классов, которые есть в большинстве наборов данных. Хорошим решением в данном случае является периодическая балансировка классов (веса класса, выборка, определенная функция потерь).</p> 
            </div>
            <hr width="100%" style="border-width: 1px;" color="#000000"/>
        </div>
    </div>
</section>

<section id="algorithm-knn">
    <div class="container">
        <div class="row">
            <div class="col-7 align-self-start">
                <p class="title-dt">kNearestNeighbors — k-ближайших соседей</p>
                <div class="description-dt">
                    <p style="text-indent: 2rem;">В алгоритме k-NN каждой точке данных (объекту) назначается класс, который является наиболее распространенным среди k соседей данного объекта, классы которых уже известны.</p> 
                    <p style="text-indent: 2rem;">В качестве функции расстояния между двумя точками в пространстве наиболее часто используется евклидово расстояние (евклидова метрика). Пусть имеются две точки в n–мерном пространстве признаков: x<sub>1</sub>=(x<sub>1,1</sub>,…,x<sub>(1,n)</sub>) и x<sub>2</sub>=(x<sub>2,1</sub>,…,x<sub>(2,n)</sub>). 
                        Евклидово расстояние между двумя точками определяется следующим образом: $$dist(x_1,x_2) = \sqrt{\sum_{i=1}^n(x_{1,i},x_{2,i})^2} .$$</p>
                    
                    <p style="text-indent: 2rem;">Число ближайших соседей k и метрика являются ключевыми компонентами алгоритма k-NN.</p>
                </div>

            </div>
            <div class="col-5 align-self-center">
                <img src="../static/img/kNN.png" alt=""  class="img-fluid">
            </div>
        </div>
        <div class="row text-center">
            <hr width="100%" color="#000000"/>
            <div class="col-7 text-center" style="margin: 0 auto;">
                <p class="title-small">Плюсы</p>
                    <p class="decr" style="text-indent: 2rem;">\(\bullet\) Алгоритм прост и его легко понять</p>
                    <p class="decr" style="text-indent: 2rem;">\(\bullet\) Тривиальное обучение модели на новых тренировочных данных</p>
                    <p class="decr" style="text-indent: 2rem;">\(\bullet\) Работает с любым количеством категорий в задаче классификации</p>
                    <p class="decr" style="text-indent: 2rem;">\(\bullet\) Легко добавить больше данных в множество данных</p>
                    <p class="decr" style="text-indent: 2rem;">\(\bullet\) Модель принимает только 2 параметра: К и метрика расстояния, которой вы хотели бы воспользоваться (обычно это Евклидово расстояние)</p>
            </div>
            <div class="col-5 text-center" style="margin: 0 auto;">
                <p class="title-small">Минусы</p>
                    <p class="decr" style="text-indent: 2rem;">\(\bullet\) Высокая стоимость вычисления, т.к. вам требуется обработать весь объем данных</p>
                    <p class="decr" style="text-indent: 2rem;">\(\bullet\) Работает не так хорошо с категорическими параметрами</p>
            </div>
            <hr width="100%" style="border-width: 1px;" color="#000000"/>
        </div>
    </div>
</section>

<section id="algorithm-nb">
    <div class="container">
        <div class="row">
            <div class="col-7 align-self-start">
                <p class="title-dt">Naive Bayes — Наивный байесовский классификатор</p>
                <div class="description-dt">
                    <p style="text-indent: 2rem;">Наивный байесовский классификатор – вероятностный классификатор, основанный на теореме Байеса, со строгими предположениями о независимости признаков объекта, от которых зависит принадлежность к классу.</p> 
                    <p style="text-indent: 2rem;">Пусть {c_1,…,c_k} – фиксированное множество классов. Необходимо найти такой класс c_j, при котором его вероятность для объекта была бы максимальна max_c⁡ p(c_j|y). 
                        Вероятность принадлежности p(c_j|y) объекта к классу вычисляется с помощью формулы Байеса для условных вероятностей: $$p(c_j|y)= \frac{p(c_j)f(y|c_j)}{\sum_{i=1}^np(c_j)f(y|c_j)} ,$$</p>
                    <p>где p(c_j|y) – вероятность получения класса с_j независимо от наблюдаемых данных, f(y|c_j) – функция плотности распределения (в дискретном случае – вероятность p(c_j|y) встретить объект y среди объектов класса c_j), знаменатель дроби – нормализующая константа.</p> 
                    <p style="text-indent: 2rem;">Таким образом, задача сводится к оценке распределения объектов для классов p(c_j) и распределений f(y|c_j) на основе тренировочных данных. Заметим, что вводится два предположения, что признаки независимы и имеют нормальные распределения внутри классов, параметры которых надо оценить.</p> 
                    <p style="text-indent: 2rem;"></p>   
                </div>
            </div>
            <div class="col-5 align-self-center">
                <img src="../static/img/NB.png" alt=""  class="img-fluid">
            </div>
        </div>
        <div class="row text-center">
            <hr width="100%" color="#000000"/>
            <div class="col-7 text-center" style="margin: 0 auto;">
                <p class="title-small">Плюсы</p>
                    <p class="decr" style="text-indent: 2rem;">\(\bullet\) Наивная байесовская классификация легко реализуема и быстра.</p>
                    <p class="decr" style="text-indent: 2rem;">\(\bullet\) Он будет сходиться быстрее, чем дискриминационные модели, такие как логистическая регрессия.</p>
                    <p class="decr" style="text-indent: 2rem;">\(\bullet\) Это требует меньше тренировочных данных.</p>
                    <p class="decr" style="text-indent: 2rem;">\(\bullet\) Он может делать вероятностные прогнозы и может обрабатывать как непрерывные, так и дискретные данные.</p>
                    <p class="decr" style="text-indent: 2rem;">\(\bullet\) Наивный байесовский алгоритм классификации можно использовать как для бинарных, так и для мультиклассовых задач классификации.</p>
            </div>
            <div class="col-5 text-center" style="margin: 0 auto;">
                <p class="title-small">Минусы</p>
                    <p class="decr" style="text-indent: 2rem;">\(\bullet\) Одним из наиболее важных минусов наивной байесовской классификации является ее сильная независимость, поскольку в реальной жизни практически невозможно иметь набор функций, которые полностью независимы друг от друга.</p>
                    <p class="decr" style="text-indent: 2rem;">\(\bullet\) Еще одна проблема, связанная с наивной байесовской классификацией, заключается в ее «нулевой частоте», которая означает, что если категориальная переменная имеет категорию, но не наблюдается в наборе обучающих данных, то наивная байесовская модель присваивает ей нулевую вероятность, и она не сможет прогнозирование.</p>
            </div>
            <hr width="100%" style="border-width: 1px;" color="#000000"/>
        </div>
    </div>
</section>

<section id="algorithm-svm">
    <div class="container">
        <div class="row">
            <div class="col-7 align-self-start">
                <p class="title-dt">Support Vector Machine — Машина опорных векторов</p>
                <div class="description-dt">
                    <p style="text-indent: 2rem;">Пусть имеются точки данных X в n-размерном пространстве информационных признаков, X=R^n, которые необходимо классифицировать на два непересекающихся класса Y={+1,-1}. Метод опорных векторов разделяет эти точки гиперплоскостью с размерностью n-1. Цель состоит в том, чтобы разделить точки данных гиперплоскостью, которая имеет максимальное расстояние до ближайшей на каждой стороне. </p> 
                    <p style="text-indent: 2rem;">Любая гиперплоскость может быть представлена в виде множества точек X, удовлетворяющих условию ω^T+b=0, где ω – нормальный вектор гиперплоскости, а b – смещение гиперплоскости ω^T+b=0 от исходной точки вдоль направления ω. Учитывая пару (ω^T,b), классифицируем данные X на два класса Y в соответствии со знаком функции f(x)=sign(ω^T,b). Линейная разделимость данных  X на два класса может быть выражена уравнением y(ω^T+b)≥1. 
                        Ближайшие к гиперплоскости точки данных называются опорными векторами. Расстояние между опорными векторами называется зазором разделителя. Линейный SVM решается путем формулирования задачи квадратичной оптимизации следующим образом: $$argmin_{ω,b}(\frac 12||ω||^2):y(ω^Tx+b)≥1$$</p>
                    <p style="text-indent: 2rem;">SVM может точно находить линейные, нелинейные и сложные границы классификации даже при небольшом размере обучающей выборки.</p> 
                </div>
            </div>
            <div class="col-5 align-self-center">
                <img src="../static/img/SVm.png" alt=""  class="img-fluid">
            </div>
        </div>
        <div class="row">
            <hr width="100%" color="#000000"/>
            <div class="col-7 text-center" style="margin: 0 auto;">
                <p class="title-small">Плюсы</p>
                    <p class="decr" style="text-indent: 2rem;">\(\bullet\) Эффективно, когда количество функций больше, чем обучающие примеры.</p>
                    <p class="decr" style="text-indent: 2rem;">\(\bullet\) Лучший алгоритм, когда классы отделимы.</p>
                    <p class="decr" style="text-indent: 2rem;">\(\bullet\) На гиперплоскость влияют только опорные векторы, поэтому выбросы оказывают меньшее влияние.</p>
                    <p class="decr" style="text-indent: 2rem;">\(\bullet\) Подходит для экстремального случая двоичной классификации.</p>
            </div>
            <div class="col-5 text-center " style="margin: 0 auto;">
                <p class="title-small">Минусы</p>
                    <p class="decr" style="text-indent: 2rem;">\(\bullet\) Для больших наборов данных требуется много времени для обработки.</p>
                    <p class="decr" style="text-indent: 2rem;">\(\bullet\) Не работает хорошо в случае перекрывающихся классов.</p>
                    <p class="decr" style="text-indent: 2rem;">\(\bullet\) Выбор, соответственно, гиперпараметров SVM, который обеспечит достаточную производительность обобщения.</p>
                    <p class="decr" style="text-indent: 2rem;">\(\bullet\) Выбор подходящей функции ядра может быть сложным.</p>
            </div>
            <hr width="100%" style="border-width: 1px;" color="#000000"/>
        </div>
    </div>
</section>

<section id="algorithm-lr" style="padding-bottom: 5rem;">
    <div class="container">
        <div class="row">
            <div class="col-7 align-self-start">
                <p class="title-dt">Logistic Regression — Логистическая регрессия</p>
                <div class="description-dt">
                    <p style="text-indent: 2rem;">Логистический регрессионный анализ изучает связь между категориально зависимой переменной и набором независимых переменных. Логистическая регрессия используется, когда зависимая переменная имеет только два значения, такие как «0 и 1» или «Да и Нет». Она конкурирует с дискриминантным анализом как методом анализа переменных категории-ответа.</p> 
                    <p style="text-indent: 2rem;">Многие статистики считают, что логистическая регрессия более универсальна и лучше подходит для моделирования большинства ситуаций, чем дискриминантный анализ. Это связано с тем, что логистическая регрессия не предполагает, что независимые переменные обычно распределяются, как и при дискриминантном анализе.</p>
                    <p style="text-indent: 2rem;">В логистической регрессии математическая модель набора объясняющих переменных используется для прогнозирования логитного преобразования зависимой переменной.</p> 
                    <p style="text-indent: 2rem;">Предположим, что двум исходам двоичной переменной присваи-ваются числовые значения 0 и 1. Часто 0 представляет отрицательную реакцию, а 1 – положительную. Среднее значение этой переменной бу-дет доля положительных ответов. Если p - это доля наблюдений с ре-зультатом 1, то 1-p – это вероятность результата 0. Соотношение p/(1-p) называется odds, а logit – логарифм odds, или просто log odds. </p>
                    <p style="text-indent: 2rem;">Математически, логитное преобразование записывается так: $$l=logit(p)=\ln\frac{p}{1-p}$$</p>
                    <p style="text-indent: 2rem;">Логистическое преобразование является обратным преобразованию логита. Оно записывается так: $$p=logistic(l)=\frac{e^l}{1+e^l}$$</p>
                </div>
            </div>
            <div class="col-5 align-self-center">
                <img src="../static/img/LR.png" alt=""  class="img-fluid">
            </div>
        </div>
        <div class="row text-center">
            <hr width="100%" color="#000000"/>
            <div class="col-7 text-center" style="margin: 0 auto">
                <p class="title-small">Плюсы</p>
                    <p class="decr" style="text-indent: 2rem;">\(\bullet\) Один из простейших алгоритмов машинного обучения обеспечивает большую эффективность.</p>
                    <p class="decr" style="text-indent: 2rem;">\(\bullet\) Низкая дисперсия.</p>
                    <p class="decr" style="text-indent: 2rem;">\(\bullet\) Может также использоваться для извлечения объектов.</p>
                    <p class="decr" style="text-indent: 2rem;">\(\bullet\) Логистические модели могут быть легко обновлены новыми данными с использованием стохастического градиентного спуска.</p>
            </div>
            <div class="col-5 text-center" style="margin: 0 auto;">
                <p class="title-small">Минусы</p>
                    <p class="decr" style="text-indent: 2rem;">\(\bullet\) Не очень хорошо обрабатывает большое количество категориальных переменных.</p>
                    <p class="decr" style="text-indent: 2rem;">\(\bullet\) Требуется преобразование нелинейных функций.</p>
                    <p class="decr" style="text-indent: 2rem;">\(\bullet\) Они не достаточно гибки, чтобы естественно охватить более сложные отношения.</p>
            </div>
        </div>
    </div>
</section>

<footer>
    <div class="container">
        <ul class="social-icons">
            <li><a href="https://vk.com/nazevg"><i class="fa fa-vk"></i></a></li>
            <li><a href="https://www.instagram.com/nazevg/"><i class="fa fa-instagram"></i></a></li>
            <li><a href="https://github.com/hurtishka"><i class="fa fa-github"></i></a></li>
        </ul>
        <p id="phone" class="footer-text">Тел.: +7 (982) 105 37-63</p>
        <p id="email" class="footer-text">Email: nazarenko674777@gmail.com</p>
        <p id="copyright" class="footer-text">© 2021, Evgenii Nazarenko</p>
    </div>
</footer>

<div id="button-up">
    <i class="fa fa-chevron-up"></i>	
</div>
<script>
    $(document).ready(function() { 
      var button = $('#button-up');	
      $(window).scroll (function () {
        if ($(this).scrollTop () > 300) {
          button.fadeIn();
        } else {
          button.fadeOut();
        }
    });	 
    button.on('click', function(){
    $('body, html').animate({
    scrollTop: 0
    }, 50);
    return false;
    });		 
    });
</script>
{% endblock %}